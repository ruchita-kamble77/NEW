spark

from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()


sc=spark.sparkContext


data=[1,2,3,4,5,6,7,8,9]


rdd=sc.parallelize(data,4)


rdd.collect()


Out[13]: [1, 2, 3, 4, 5, 6, 7, 8, 9]

rdd.glom().collect()
Out[14]: [[1, 2], [3, 4], [5, 6], [7, 8, 9]]
result=rdd.count()
rdd1=sc.parallelize([1,2,3,4,5,6])
result=rdd1.filter(lambda x:x%2 == 0)
print(result.collect())
[2, 4, 6]


rdd2=sc.parallelize([1,2,3])
result=rdd2.flatMap(lambda x:(x,x*2))
print(result.collect())
[1, 2, 2, 4, 3, 6]


rdd3=sc.parallelize([('a',1),('b',2),('a',3)])
result=rdd3.reduceByKey(lambda x,y:x+y)
print(result.collect())
[('a', 4), ('b', 2)]


rdd4=sc.parallelize([('a',1),('b',2),('a',3),('b',4)])
result=rdd4.groupByKey().mapValues(list)
print(result.collect())
[('a', [1, 3]), ('b', [2, 4])]


rdd5=sc.parallelize([('a',1),('b',2)])
rdd6=sc.parallelize([('a',3),('b',4)])
result= rdd5.join(rdd6)
print(result.collect())
[('b', (2, 4)), ('a', (1, 3))]


rdd7=sc.parallelize([1,2,3,2,1])
result=rdd7.distinct()
print(result.collect())
[1, 2, 3]
#rdd action
rdd01=sc.parallelize([1,2,3,4])
result=rdd01.collect()
print(result)
[1, 2, 3, 4]


rdd01=sc.parallelize([1,2,3,4])
result=rdd01.count()
print(result)
4


rdd03=sc.parallelize([1,2,3,4])
result=rdd03.reduce(lambda x,y:x+y)
print(result)
10


result=rdd01.first()
print(result)
1


result=rdd01.take(3)
print(result)
[1, 2, 3]
